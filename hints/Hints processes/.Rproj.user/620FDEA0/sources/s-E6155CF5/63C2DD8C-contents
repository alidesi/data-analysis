

#Hints 2: hint harder

x=NULL
for (i in 21:29)
{
  x = rbind(x,read.csv(paste0("C:/Users/reedm/UK Parliament/THOMAS, Liz - DAs/Matt R/searchTerms/query_data (",i,").csv"), stringsAsFactors=F ))

}
names(x) = c("Date",names(x)[2:length(x)])

d = subset(x,Date == "2018-07-19T00:00:00Z")
x2 = ddply(x, .(Date), function(y)sum(y$Count) )

x$SearchTerm = tolower(trimws(x$SearchTerm))
x3 = ddply(x, .(SearchTerm), function(x)x[1,] )
x3=x3[order(x3$SearchTerm),]

#write.csv(x3,"C:/Users/reedm/UK Parliament/Data & Search - Data and Search (Public)/Search/Result text/BigSearchTermsList.csv", row.names=F)



ggplot(x2, aes(x=Date,y=V1))+geom_bar(stat="identity")+coord_flip()

####################### data then run through powershell, then read it back in here.

library(jsonlite)


bigJson <-read_json("C:/Users/reedm/UK Parliament/Data & Search - Data and Search (Public)/Search/Result text/searchSummaryTextMatt.json")

#s=head(bigJson)

df1 = list()
for (name in names(bigJson[[1]])){
  df1[[name]] = sapply(bigJson, function(x)as.character(x[name]))
}
df2 = as.data.frame(df1)


names(df2) = c("contentText","SearchTerm","titleText","uri")


searchesAndFirstPage = merge(x,df2 )
searchesAndFirstPage$contentText = as.character(searchesAndFirstPage$contentText)
searchesAndFirstPage$titleText = as.character(searchesAndFirstPage$titleText)
searchesAndFirstPage$uri = as.character(searchesAndFirstPage$uri)

# links not matched
ggplot(subset(x, !(SearchTerm %in% searchesAndFirstPage$SearchTerm)),aes(x=Date))+geom_bar(data=x,fill="red")+geom_bar()+coord_flip()


# write.csv(searchesAndFirstPage, "C:/Users/reedm/UK Parliament/Data & Search - Data and Search (Public)/Search/Result text/termsAndResults.csv")

folder = "C:/Users/reedm/UK Parliament/THOMAS, Liz - DAs/Hints/"
# rules <- read.csv(paste(folder,"Rules.csv",sep=""))
# uRules <- read.csv(paste(folder,"unformatRules.csv",sep=""), stringsAsFactors = F)[,1]
# uRules <- uRules[!grepl('#',uRules)]
# uRules2 <- strsplit(uRules[uRules != "-"], ' ')
# uRules3 <- laply(uRules2,function(x)x[1])
# rules2 <- data.frame(rule=substr(uRules3, 2, nchar(uRules3)),ruleName=laply(uRules2, function(x)paste(x[2:length(x)],collapse = ' ') ) )
# rules2 <- rules2[1:(nrow(rules2)-2),]
# names(rules) <- c("rule","ruleName")
# r3 <- rbind(rules2, rules)
# rules <- unique(r3)
#   rules$rule <- as.character(rules$rule)
# #rules$rule <- gsub("\\?<filter>","" ,rules$rule)
# #rules[substr(rules$rule,1,1) == "(" & substr(rules$rule,nchar(rules$rule),nchar(rules$rule)) == ")",]$rule = # get rid of brackets at start and end
# #  substr(rules[substr(rules$rule,1,1) == "(" & substr(rules$rule,nchar(rules$rule),nchar(rules$rule)) == ")",]$rule,2,
# #         nchar(rules[substr(rules$rule,1,1) == "(" & substr(rules$rule,nchar(rules$rule),nchar(rules$rule)) == ")",]$rule)-1)
# rules$rule = trimws(rules$rule)
# rules$ruleName = trimws(rules$ruleName)
# rules = unique(rules)
#
# write.csv(rules, paste0(folder, "bigRulesList.csv"), row.names=F)

rules = read.csv(paste0("C:/Users/reedm/UK Parliament/Data & Search - Data and Search (Public)/Search/Hints/","ReferenceHints.csv"))
rules$rule <- gsub("\\?<filter>","" ,rules$rule)
rules$ruleName = gsub("\\?<filter>","",rules$ruleName)

# this is the big match part, and takes a while

# first time i ran this, certain entries were overwritten. rerunning it a second time to pick up those that were got rid of the first time.
# the names of the df habe been altered now, so this wont be necessary

#runAgain = subset(rules, ruleName %in% allMatches$ruleName & !(rule %in% allMatches$regex))

bigList = list()
count = 1
for (i in 1:nrow(rules))#1:nrow(runAgain))#which(rules$rule %in% currentLinks$regex))#1:nrow(rules))
{
  rule = rules$rule[i]#runAgain$rule[i]#rules$rule[i]
  ruleName = rules$ruleName[i]
  s = regmatches(searchesAndFirstPage$uri, regexec(rule,searchesAndFirstPage$uri) )
  groups = s[which(lengths(s) > 0)]
  if (length(groups) > 0)
  {
  forDF = searchesAndFirstPage[which(lengths(s) > 0),]
  matches =  forDF$uri
  ruleNames = rep(ruleName, length(matches))

  if (grepl("\\{",ruleName))
  {
    for (j in 1:lengths(regmatches(ruleName, gregexpr("\\{", ruleName))))
    {
      replacements = sapply(groups, function(x)x[j+1])
      ruleNames = as.character(mapply(gsub, paste0("\\{",j,"\\}"), replacements, ruleNames))
    }
  }

  df = data.frame(uri=matches,ruleName=ruleNames,regex=rule,SearchTerm=forDF$SearchTerm,Date = forDF$Date, 
                  Count = forDF$Count, contentText=  forDF$contentText,
                  titleText=  forDF$titleText)
  bigList[[paste(rule,"-",ruleName)]] = df
  }
  print(paste(rule,"-",ruleName,count))
  count=count+1
}
y=do.call(rbind, bigList)

 
# write.csv(y, paste0(folder, "AllHintsMatched0712to0805 2.csv"),row.names=F)






# allMatches <- read.csv(paste0(folder, "AllHintsMatched0712to0805.csv"),stringsAsFactors = F)
# allMatches2 <- read.csv(paste0(folder, "AllHintsMatched0712to0805 2.csv"),stringsAsFactors = F)
# allMatches = rbind(allMatches, allMatches2)
# allMatches$ruleName=trimws(allMatches$ruleName)
# allMatches = unique(allMatches)
# write.csv(allMatches, paste0(folder, "AllHintsMatchedCLeaner.csv") )

folder = "C:/Users/reedm/UK Parliament/THOMAS, Liz - DAs/Hints/"
rules = read.csv(paste0(folder, "bigRulesList.csv"), stringsAsFactors = F)
searchesAndFirstPage <- read.csv("C:/Users/reedm/UK Parliament/Data & Search - Data and Search (Public)/Search/Result text/termsAndResults.csv", stringsAsFactors = F)
searchesAndFirstPage$X = NULL

allMatches = read.csv(paste0(folder, "AllHintsMatchedCLeaner.csv") , stringsAsFactors = F)
allMatches$X = NULL
links <- read.csv("C:/Users/reedm/UK Parliament/Data & Search - Data and Search (Public)/Search/Result text/termsAndResults.csv",stringsAsFactors = F)

## how does our previous selection of links work with this set?

currentLinks <- readLines("C:/Users/reedm/UK Parliament/THOMAS, Liz - DAs/Hints/Rules.tsv" )
a1 = strsplit(currentLinks, '\t')
currentLinks = data.frame(regex = sapply(a1,function(x)x[1]), ruleName = sapply(a1,function(x)x[2]))

uniqueMatchedUris = (unique(allMatches$uri))
uniqueAllUris = unique(searchesAndFirstPage$uri)

# total coverage with all rules
length(uniqueMatchedUris)/length(uniqueAllUris)

# total coverage with the current hint set
length(unique(subset(allMatches, regex %in% currentLinks$regex)$uri))/length(uniqueAllUris)


currentlyCovered = subset(allMatches, regex %in% currentLinks$regex)


uniqueMatches = unique(allMatches[,c("uri","regex","ruleName")])
x2=as.data.frame(table(uniqueMatches$regex))
x2=merge(x2, rules, by.x="Var1",by.y="rule")
x2=x2[order(-x2$Freq),]

# write an algorithm that ensure no more than 2 tags per entry


# the below loop creates a maximal list of 

uniqueUri = data.frame(uri = unique(uniqueMatches$uri))
coverage = 0
maximumCover = 12
newList= list()
thisOne = NULL
uniqueUri$coverage = 0
keeps = data.frame(Var1 = character(),Freq=numeric(),ruleName=character(), cumulative=numeric())
while (coverage < length(uniqueMatchedUris)/length(uniqueAllUris))
{
  matchesbanned = subset(allMatches, uri %in% subset(uniqueUri, coverage >= maximumCover)$uri)
  
  x2=as.data.frame(table(subset(uniqueMatches, !(regex %in% matchesbanned$regex) & !(regex %in% thisOne$regex) & !(uri %in% thisOne$uri) )$regex))
  x2=merge(x2, rules, by.x="Var1",by.y="rule")
  x2=x2[order(-x2$Freq),]
  
  thisRule = x2[1,]
  
  uniqueUri[uniqueUri$uri %in% subset(uniqueMatches, regex== thisRule$Var1 )$uri,]$coverage = 
      uniqueUri[uniqueUri$uri %in% subset(uniqueMatches, regex== thisRule$Var1 )$uri,]$coverage + 1
  if (!exists("thisOne"))
    thisOne = subset(uniqueMatches, regex == thisRule$Var1 )
  else
    thisOne = rbind(thisOne, subset(uniqueMatches, regex == thisRule$Var1 ))
  coverage = length(unique(thisOne$uri)) / nrow(uniqueUri)
  print(paste(thisRule$Var1[1], thisRule$ruleName[1], "-", coverage))
  thisRule$cumulative = coverage
  keeps=rbind(keeps, thisRule)
}


row.names(keeps) = NULL
keeps$name= paste(row.names(keeps),keeps$ruleName)


ggplot(keeps, aes(x=reorder(name, cumulative), y=cumulative))+geom_point()+theme(axis.text.x= element_text(angle =20,hjust=1  ))









